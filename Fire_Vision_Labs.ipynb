{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "GCS_OUTPUT = 'gs://cloudfire_lyrical-edition-273206/fire_dataset/tfrecords-dataset-eval/'  # prefix for output file names\n",
    "\n",
    "TARGET_SIZE = [100, 100]\n",
    "\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
    "\n",
    "        # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.string),  # one bytestring\n",
    "        \"size\": tf.io.FixedLenFeature([2], tf.int64),  # two integers\n",
    "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32)  # a certain number of floats\n",
    "    }\n",
    "    # decode the TFRecord\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "\n",
    "    # FixedLenFeature fields are now ready to use: exmple['size']\n",
    "    # VarLenFeature fields require additional sparse_to_dense decoding\n",
    "\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.reshape(image, [*TARGET_SIZE, 3])\n",
    "\n",
    "    class_num = example['class']\n",
    "\n",
    "    label = example['label']\n",
    "    height = example['size'][0]\n",
    "    width = example['size'][1]\n",
    "    one_hot_class = tf.sparse.to_dense(example['one_hot_class'])\n",
    "    return image, class_num, label, height, width, one_hot_class\n",
    "\n",
    "\n",
    "# read from TFRecords. For optimal performance, read from multiple\n",
    "# TFRecord files at once and set the option experimental_deterministic = False\n",
    "# to allow order-altering optimizations.\n",
    "\n",
    "option_no_order = tf.data.Options()\n",
    "option_no_order.experimental_deterministic = False\n",
    "\n",
    "filenames = tf.io.gfile.glob(GCS_OUTPUT + \"*.tfrec\")\n",
    "dataset4 = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "dataset4 = dataset4.with_options(option_no_order)\n",
    "dataset4 = dataset4.map(read_tfrecord, num_parallel_calls=AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, class_num, label, height, width, one_hot_class in dataset4.take(25):\n",
    "    print(\"Image shape {}, {}x{} px, class={} ({:>10}, {})\".format(image.numpy().shape, width, height, class_num,label.numpy().decode('utf8'),one_hot_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "CLASSES = ['Fire', 'Normal']\n",
    "\n",
    "def dataset_to_numpy_util(dataset, N):\n",
    "    dataset = dataset.batch(N)\n",
    "    for images, labels in dataset:\n",
    "        numpy_images = images.numpy()\n",
    "        numpy_labels = labels.numpy()\n",
    "        break;  \n",
    "    return numpy_images, numpy_labels\n",
    "'''\n",
    "def title_from_label_and_target(label, correct_label):\n",
    "    label = np.argmax(label, axis=-1)  # one-hot to class number\n",
    "    correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n",
    "    correct = (label == correct_label)\n",
    "    return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
    "                                CLASSES[correct_label] if not correct else ''), correct\n",
    "'''\n",
    "def display_one_flower(image, title, subplot, red=False):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
    "    return subplot+1\n",
    "  \n",
    "def display_9_images_from_dataset(dataset):\n",
    "    subplot=331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    images, labels = dataset_to_numpy_util(dataset, 9)\n",
    "    for i, image in enumerate(images):\n",
    "        title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
    "        subplot = display_one_flower(image, title, subplot)\n",
    "        if i >= 8:\n",
    "            break;\n",
    "              \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()  \n",
    "\n",
    "    '''\n",
    "def display_9_images_with_predictions(images, predictions, labels):\n",
    "    subplot=331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    for i, image in enumerate(images):\n",
    "        title, correct = title_from_label_and_target(predictions[i], labels[i])\n",
    "        subplot = display_one_flower(image, title, subplot, not correct)\n",
    "        if i >= 8:\n",
    "            break;\n",
    "              \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_9_images_from_dataset(dataset4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "CLASSES = ['Fire', 'Normal']\n",
    "\n",
    "def show_batch(numpy_images, numpy_labels):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(25):\n",
    "        plt.subplot(3, 3, 8)\n",
    "        plt.imshow(numpy_images[n])\n",
    "        plt.title(CLASSES[numpy_labels[n]==1][0].title())\n",
    "        plt.axis('off')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_numpy_util(dataset4,N):\n",
    "    dataset = dataset4.batch(N)\n",
    "    for image, _, label, _, _, _ in dataset:\n",
    "        numpy_images = image.numpy()\n",
    "        numpy_labels = label.numpy() \n",
    "        show_batch(numpy_images, numpy_labels)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_numpy_util(dataset4, 25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m cp -r /home/jupyter/FYP_FIRENET/fire-eye gs://cloudfire_lyrical-edition-273206/trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_DIR=/home/jupyter/FYP_FIRENET/fire-eye\n",
    "#rm -rf Trainer.tar.gz fire-eye\n",
    "gcloud ai-platform local train \\\n",
    "    --module-name=Trainer.config \\\n",
    "    --package-path=${PWD}/Trainer \\\n",
    "    -- \\\n",
    "    --output_dir=$MODEL_DIR\\\n",
    "    --train_steps=50 \\\n",
    "    --learning_rate=0.001 \\\n",
    "    --batch_size=20 \\\n",
    "    --train_data_path=gs://cloudfire_lyrical-edition-273206/fire_dataset/tfrecords-dataset-7/ \\\n",
    "    --eval_data_path=gs://cloudfire_lyrical-edition-273206/fire_dataset/tfrecords-dataset-eval/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir /home/jupyter/.kaggle/\n",
    "mv /home/jupyter/kaggle.json /home/jupyter/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kaggle datasets download --unzip -d phylake1337/fire-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mv /home/jupyter/FYP_FIRENET/fire_dataset /home/jupyter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m mv -r /home/jupyter/fire_images/*.jpg gs://cloudfire_lyrical-edition-273206/fire_dataset/block3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m mv -r /home/jupyter/non_fire_images/*.jpg gs://cloudfire_lyrical-edition-273206/fire_dataset/Normal/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m cp -r /home/jupyter/FYP_FIRENET/fire-eye gs://cloudfire_lyrical-edition-273206/model_dir01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd home/jupyter/FYP_FIRENET\n",
    "python preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI platform Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- setup env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"citric-sol-273815\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"cloudfire_lyrical-edition-273206\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "WARNING: You do not appear to have access to project [citric-sol-273815] or it does not exist.\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This allows ai platform to read/write to the staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This command will fail if the Cloud Machine Learning Engine API is not enabled using the link above.\n",
    "echo \"Getting the service account email associated with the Cloud AI Platform API\"\n",
    "\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print (response['serviceAccount'])\")  # If this command fails, the Cloud Machine Learning Engine API has not been enabled above.\n",
    "\n",
    "echo \"Authorizing the Cloud AI Platform account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET   \n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET   # error message (if bucket is empty) can be ignored.  \n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- submits training job to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloudfire_lyrical-edition-273206/model_dir77 us-central1 FireSage_200605_205257\n",
      "jobId: FireSage_200605_205257\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [FireSage_200605_205257] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe FireSage_200605_205257\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs FireSage_200605_205257\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://cloudfire_lyrical-edition-273206/model_dir77\n",
    "REGION=\"us-central1\"\n",
    "JOBNAME=FireSage_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "# gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=Trainer.config \\\n",
    "    --package-path=${PWD}/Trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://cloudfire_lyrical-edition-273206 \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=2.1 \\\n",
    "    --python-version=3.7 \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=500 \\\n",
    "    --learning_rate=0.001 \\\n",
    "    --batch_size=40 \\\n",
    "    --train_data_path=gs://cloudfire_lyrical-edition-273206/fire_dataset/tfrecords-dataset-7/ \\\n",
    "    --eval_data_path=gs://cloudfire_lyrical-edition-273206/fire_dataset/tfrecords-dataset-eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
