{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "GCS_OUTPUT = 'gs://cloudfire_citric-sol_5670/fire_dataset/tfrecords-dataset-5x/'  # prefix for output file names\n",
    "\n",
    "TARGET_SIZE = [224, 224]\n",
    "\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
    "\n",
    "        # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.string),  # one bytestring\n",
    "        \"size\": tf.io.FixedLenFeature([2], tf.int64),  # two integers\n",
    "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32)  # a certain number of floats\n",
    "    }\n",
    "    # decode the TFRecord\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "\n",
    "    # FixedLenFeature fields are now ready to use: exmple['size']\n",
    "    # VarLenFeature fields require additional sparse_to_dense decoding\n",
    "\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.reshape(image, [*TARGET_SIZE, 3])\n",
    "\n",
    "    class_num = example['class']\n",
    "\n",
    "    label = example['label']\n",
    "    height = example['size'][0]\n",
    "    width = example['size'][1]\n",
    "    one_hot_class = tf.sparse.to_dense(example['one_hot_class'])\n",
    "    return image, class_num, label, height, width, one_hot_class\n",
    "\n",
    "\n",
    "# read from TFRecords. For optimal performance, read from multiple\n",
    "# TFRecord files at once and set the option experimental_deterministic = False\n",
    "# to allow order-altering optimizations.\n",
    "\n",
    "option_no_order = tf.data.Options()\n",
    "option_no_order.experimental_deterministic = False\n",
    "\n",
    "filenames = tf.io.gfile.glob(GCS_OUTPUT + \"*.tfrec\")\n",
    "dataset4 = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "dataset4 = dataset4.with_options(option_no_order)\n",
    "dataset4 = dataset4.map(read_tfrecord, num_parallel_calls=AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, class_num, label, height, width, one_hot_class in dataset4.take(100):\n",
    "    print(\"Image shape {}, {}x{} px, class={} ({:>10}, {})\".format(image.numpy().shape, width, height, class_num,label.numpy().decode('utf8'),one_hot_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "CLASSES = ['Fire', 'Normal']\n",
    "\n",
    "def dataset_to_numpy_util(dataset, N):\n",
    "    dataset = dataset.batch(N)\n",
    "    for images, labels in dataset:\n",
    "        numpy_images = images.numpy()\n",
    "        numpy_labels = labels.numpy()\n",
    "        break;  \n",
    "    return numpy_images, numpy_labels\n",
    "'''\n",
    "def title_from_label_and_target(label, correct_label):\n",
    "    label = np.argmax(label, axis=-1)  # one-hot to class number\n",
    "    correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n",
    "    correct = (label == correct_label)\n",
    "    return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
    "                                CLASSES[correct_label] if not correct else ''), correct\n",
    "'''\n",
    "def display_one_flower(image, title, subplot, red=False):\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
    "    return subplot+1\n",
    "  \n",
    "def display_9_images_from_dataset(dataset):\n",
    "    subplot=331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    images, labels = dataset_to_numpy_util(dataset, 9)\n",
    "    for i, image in enumerate(images):\n",
    "        title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
    "        subplot = display_one_flower(image, title, subplot)\n",
    "        if i >= 8:\n",
    "            break;\n",
    "              \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()  \n",
    "\n",
    "    '''\n",
    "def display_9_images_with_predictions(images, predictions, labels):\n",
    "    subplot=331\n",
    "    plt.figure(figsize=(13,13))\n",
    "    for i, image in enumerate(images):\n",
    "        title, correct = title_from_label_and_target(predictions[i], labels[i])\n",
    "        subplot = display_one_flower(image, title, subplot, not correct)\n",
    "        if i >= 8:\n",
    "            break;\n",
    "              \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_9_images_from_dataset(dataset4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "CLASSES = ['Fire', 'Normal']\n",
    "\n",
    "def show_batch(numpy_images, numpy_labels):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(25):\n",
    "        plt.subplot(5, 5, n+1)\n",
    "        plt.title(CLASSES[np.argmax(numpy_labels[n], axis=0)])\n",
    "        plt.imshow(numpy_images[n])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_numpy_util(dataset4,N):\n",
    "    dataset = dataset4.batch(N)\n",
    "    for image, _, label, _, _, _ in dataset:\n",
    "        numpy_images = image.numpy()\n",
    "        numpy_labels = label.numpy() \n",
    "        show_batch(numpy_images, numpy_labels)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_numpy_util(dataset4, 25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m cp -r /home/jupyter/FYP_FIRENET/fire-eye gs://cloudfire_lyrical-edition-273206/trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_DIR=/home/jupyter/FYP_FIRENET/fire-eye-test77\n",
    "rm -rf Trainer.tar.gz fire-eye\n",
    "gcloud ai-platform local train \\\n",
    "    --module-name=Trainer.config \\\n",
    "    --package-path=${PWD}/Trainer \\\n",
    "    -- \\\n",
    "    --output_dir=$MODEL_DIR\\\n",
    "    --train_steps=500 \\\n",
    "    --learning_rate=0.001 \\\n",
    "    --batch_size=20 \\\n",
    "    --train_data_path=gs://cloudfire_citric-sol_5670/fire_dataset/tfrecords-dataset-*x/ \\\n",
    "    --eval_data_path=gs://cloudfire_citric-sol_5670/fire_dataset/tfrecords-dataset-eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tensorboard --logdir /home/jupyter/FYP_FIRENET/fire-eye-test77 --port 8000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir /home/jupyter/.kaggle/\n",
    "mv /home/jupyter/kaggle.json /home/jupyter/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kaggle datasets download --unzip -d phylake1337/fire-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mv /home/jupyter/FYP_FIRENET/fire_dataset /home/jupyter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m mv -r /home/jupyter/fire_images/*.jpg gs://cloudfire_lyrical-edition-273206/fire_dataset/block3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m cp -r /home/jupyter/tfrecords-dataset-5xx gs://cloudfire_citric-sol_5670/fire_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m cp -r /home/jupyter/fire_dataset/ gs://firetemple_avid-glass-5199/tb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd home/jupyter/FYP_FIRENET\n",
    "python preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI platform Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- setup env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"citric-sol-273815\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"cloudfire_lyrical-edition-273206\" # REPLACE WITH YOUR BUCKET NAME\n",
    "MBUCKET = \"firetemple_avid-glass-5199\"\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"MBUCKET\"] = MBUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This allows ai platform to read/write to the staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This command will fail if the Cloud Machine Learning Engine API is not enabled using the link above.\n",
    "echo \"Getting the service account email associated with the Cloud AI Platform API\"\n",
    "\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print (response['serviceAccount'])\")  # If this command fails, the Cloud Machine Learning Engine API has not been enabled above.\n",
    "\n",
    "echo \"Authorizing the Cloud AI Platform account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$MBUCKET   \n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$MBUCKET   # error message (if bucket is empty) can be ignored.  \n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$MBUCKET   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- submits training job to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://firetemple_avid-glass-5199/tb-event_files-aipf\n",
    "REGION=\"us-west1\"\n",
    "TRAIN_DATA=gs://cloudfire_citric-sol_5670/fire_dataset/tfrecords-dataset-*x/\n",
    "EVAL_DATA=gs://cloudfire_citric-sol_5670/fire_dataset/tfrecords-dataset-evalx/\n",
    "JOBNAME=FireSage_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "# gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training FireSage_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --job-dir $OUTDIR \\\n",
    "    --runtime-version 2.1 \\\n",
    "    --python-version 3.7 \\\n",
    "    --module-name Trainer.config \\\n",
    "    --package-path ${PWD}/Trainer/ \\\n",
    "    --region $REGION \\\n",
    "    --scale-tier BASIC_GPU \\\n",
    "    -- \\\n",
    "    --output_dir $OUTDIR \\\n",
    "    --train_data_path $TRAIN_DATA \\\n",
    "    --eval_data_path $EVAL_DATA \\\n",
    "    --train_steps 500 \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --batch_size 50\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ai-platform jobs submit training --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
    "    activations = get_activations([X_batch,0]) \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "print('Beginning file download with requests')\n",
    "\n",
    "url = 'https://www.floydhub.com/api/v1/resources/RwGobMM8FhzX9sUcgnBJTa?content=true&download=true&rename=ninaduf-datasets-no_fire-1'\n",
    "r = requests.get(url)\n",
    "\n",
    "with open('/home/jupyter/ninaduf-datasets-nofire-1.tar', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Retrieve HTTP meta-data\n",
    "print(r.status_code)\n",
    "print(r.headers['content-type'])\n",
    "print(r.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "data_path = '/home/jupyter/Normal'\n",
    "data_file = '/home/jupyter/ninaduf-datasets-nofire-1.tar'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    with tarfile.open(data_file) as tar:\n",
    "        tar.extractall(path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6710\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/jupyter/block5\n",
    "find | wc -l\n",
    "# ls  *.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mv  /home/jupyter/block5/Normal/*.jpg /home/jupyter/Test/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
